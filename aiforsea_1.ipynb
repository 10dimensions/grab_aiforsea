{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aiforsea_1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/10dimensions/grab_aiforsea/blob/master/aiforsea_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtT9cL6OSStX",
        "colab_type": "code",
        "outputId": "0b6fd951-5e86-4419-f492-e8dd1aa99287",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install pygeohash\n",
        "#!pip install geohash2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pygeohash in /usr/local/lib/python3.6/dist-packages (1.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW_0atMC_CQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import pygeohash\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/10dimensions/grab_aiforsea/master/training.csv'\n",
        "df = pd.read_csv(url, chunksize = 10000)\n",
        "\n",
        "train_dataset = pd.DataFrame();\n",
        "test_dataset = pd.DataFrame();\n",
        "\n",
        "train_labels = pd.DataFrame();\n",
        "test_labels = pd.DataFrame();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OSShXkPbC8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gh_decode(hash):\n",
        "    lat, lon = pygeohash.decode(hash)\n",
        "    return pd.Series({\"latitude\":float(lat), \"longitude\":float(lon)})\n",
        "\n",
        "for chunk_df in df:\n",
        "    \n",
        "    chunk_df = chunk_df.join(chunk_df[\"geohash6\"].apply(gh_decode))\n",
        "    chunk_df['day'] = chunk_df['day'] % 7\n",
        "    chunk_df['timestamp'] = chunk_df['timestamp'].str.split(':').apply(lambda x: ( int(x[0])*60 + int(x[1])) / 10 )\n",
        "    chunk_df['demand'] = chunk_df['demand'] * 1000\n",
        "    \n",
        "    chunk_df = chunk_df.drop(['geohash6'], axis=1)\n",
        "    \n",
        "    train_chunk = chunk_df.sample(frac=0.8,random_state=0)\n",
        "    test_chunk = chunk_df.drop(train_chunk.index)\n",
        "    \n",
        "    train_dataset = train_dataset.append( train_chunk )\n",
        "    test_dataset = test_dataset.append( test_chunk )    \n",
        "    \n",
        "    #chunk_df.head()\n",
        "    #chunk_result.to_csv(output_path, mode='a', header=False)\n",
        "    \n",
        "train_labels = train_dataset.pop('demand')\n",
        "test_labels = test_dataset.pop('demand')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCC2wARK773Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.Sequential([\n",
        "    layers.Dense(32, activation=tf.nn.relu, input_shape=[len(train_dataset.keys())]),\n",
        "    layers.Dense(32, activation=tf.nn.relu),\n",
        "    layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
        "\n",
        "  model.compile(loss='mean_squared_error',\n",
        "                optimizer=optimizer,\n",
        "                metrics=['mean_absolute_error', 'mean_squared_error'])\n",
        "  return model\n",
        "\n",
        "model = build_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRMeJsgzMPPQ",
        "colab_type": "code",
        "outputId": "a1b2546f-4128-45db-873c-2e38315262a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Display training progress by printing a single dot for each completed epoch\n",
        "class PrintDot(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs):\n",
        "    if epoch % 100 == 0: print('')\n",
        "    print('.', end='')\n",
        "\n",
        "EPOCHS = 25\n",
        "\n",
        "history = model.fit(\n",
        "  train_dataset, train_labels,\n",
        "  epochs=EPOCHS, validation_split = 0.2, verbose=0,\n",
        "  callbacks=[PrintDot()])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "........................."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW9PQHr2N47r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loss, mae, mse = model.evaluate(train_dataset, test_labels, verbose=0)\n",
        "\n",
        "#print(\"Testing set Mean Abs Error: {:5.2f} MPG\".format(mae))\n",
        "\n",
        "\n",
        "test_predictions = model.predict(test_dataset).flatten()\n",
        "error = test_predictions - test_labels"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}